
{
    "model_class": "BidirectionalAttentionFlow",
    "model_serialization_prefix": "/home/marilena/Dropbox/ONTOLOGY-BUILDING/RESOURCES-DEV/deep_qa_models/bidaf",
    "encoder": {
        "word": {
          "type": "cnn",
          "ngram_filter_sizes": [5],
          "num_filters": 100
        }
    },
    "seq2seq_encoder": {
        "default": {
            "type": "bi_gru",
            "encoder_params": {
                "units": 100
            },
            "wrapper_params": {}
        }
    },
    "data_generator": {
      "dynamic_padding": true,
      "adaptive_batch_sizes": true,
      "adaptive_memory_usage_constant": 440000,
      "maximum_batch_size": 60
    },
    // This is not quite the same as Min's paper; we don't have encoder dropout yet.
    "patience": 3,
    "embeddings": {
      "words": {
        "dimension": 100,
        "pretrained_file": "/home/marilena/Dropbox/ONTOLOGY-BUILDING/RESOURCES-DEV/glove/glove.6B.100d.txt.gz",
        "project": true,
        "fine_tune": false,
        "dropout": 0.2
      },
      "characters": {
        "dimension": 8,
        "dropout": 0.2
      }
    },
    "num_epochs": 20,
    "optimizer": {
      "type": "adadelta",
      "learning_rate": 0.5
    },
    "validation_files": ["/home/marilena/Dropbox/ONTOLOGY-BUILDING/RESOURCES-DEV/squad/processed/dev.tsv"],
    "train_files": ["/home/marilena/Dropbox/ONTOLOGY-BUILDING/RESOURCES-DEV/squad/processed/train.tsv"]
}
